{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importation"
      ],
      "metadata": {
        "id": "nV_PTP7DwidQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPSZDYDpwe6f"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "import torch\n",
        "import mmap"
      ],
      "metadata": {
        "id": "YuE9aPKWwxUd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o00YzE8Xx582",
        "outputId": "e556f89b-2ed3-4100-e41a-9fbb9502a32f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "FVt_FG5wAiBp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparamters"
      ],
      "metadata": {
        "id": "M2LNVvROy8Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 3000\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 250\n",
        "n_embd = 384\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "1uqOil6qy-Nv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "1U-pOuyjxKLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"glaiveai/glaive-code-assistant\")\n",
        "print(\"Dataset loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUDKtfc7xXmK",
        "outputId": "0a3dd4a1-a581-4f17-c0bf-c9925fc3036b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "train_data = train_test_split['train']\n",
        "val_data = train_test_split['test']\n",
        "\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7cNdx5ByfnA",
        "outputId": "839d5e21-d2ba-454b-c9b5-0cdf517131d5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 108887\n",
            "Validation set size: 27222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_data\n",
        "val_dataset = val_data\n",
        "\n",
        "print(\"Train sample:\", train_dataset[0])\n",
        "print(\"Validation sample:\", val_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKAk7GB9ykdz",
        "outputId": "4d355a03-82b7-4c9f-ed64-3131f2697b3a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sample: {'question': 'I have a list of numbers and I want to calculate the sum of all the numbers. Can someone provide me with a code example using C programming language?', 'answer': 'Certainly! Here\\'s a code example in C that calculates the sum of a given list of numbers:\\n\\n```c\\n#include<stdio.h>\\n\\nint main()\\n{\\n    int array[] = {1, 10, 100};\\n    int size = 3;\\n    int sum = 0;\\n    \\n    for (int i = 0; i < size; i++) {\\n        sum = sum + array[i];\\n    }\\n    \\n    printf(\"Sum of the array numbers is %d\", sum);\\n\\n    return 0;\\n}\\n```\\n\\nIn this code, we first declare an array called `array` with three elements: 1, 10, and 100. The variable `size` is set to 3 to represent the number of elements in the array.\\n\\nWe then initialize a variable `sum` to 0, which will store the sum of the numbers.\\n\\nNext, we use a `for` loop to iterate through each element of the array. The loop starts with `i` as 0 and continues until `i` is less than `size`. In each iteration, we add the current element of the array to the `sum` variable.\\n\\nFinally, we use `printf` to print out the sum of the numbers. The `%d` format specifier is used to print an integer.\\n\\nWhen you run this program, it will print: \"Sum of the array numbers is 111\", as the sum of 1, 10, and 100 is 111.\\n\\nI hope this helps! Let me know if you have any further questions.'}\n",
            "Validation sample: {'question': 'How can I implement a function in Python that can find the middle element of a given array?', 'answer': \"You can implement a function that returns the middle element of an array using the following code:\\n\\n```python\\ndef get_middle_element(arr):\\n    if len(arr) % 2 == 0:\\n        mid = int(len(arr)/2)\\n        middle = (arr[mid]+arr[mid-1]) / 2\\n        return middle\\n    else:\\n        mid = int(len(arr)/2)\\n        middle = arr[mid]\\n        return middle\\n\\narr = [1, 2, 3, 4, 5]\\nresult = get_middle_element(arr)\\nprint(result)\\n```\\n\\nThis implementation checks if the length of the array is even or odd. If it's even, the code calculates the average of the two middle elements by adding them together and dividing by 2. If it's odd, it simply returns the middle element. In the provided example, the array `[1, 2, 3, 4, 5]` has a length of 5, so the middle element is 3.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'sep_token': '[SEP]'})\n",
        "\n",
        "def convert_to_tensor_batch(dataset, batch_size=32):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = [dataset[j] for j in range(i, min(i + batch_size, len(dataset)))]\n",
        "\n",
        "        texts = [entry['question'] + \" [SEP] \" + entry['answer'] for entry in batch]\n",
        "\n",
        "        encoding = tokenizer(texts,\n",
        "                             return_tensors=\"pt\",\n",
        "                             padding='max_length',\n",
        "                             truncation=True,\n",
        "                             max_length=block_size,\n",
        "                             add_special_tokens=True)\n",
        "\n",
        "        input_ids.append(encoding['input_ids'])\n",
        "        attention_masks.append(encoding['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "train_input_ids, train_attention_masks = convert_to_tensor_batch(train_dataset)\n",
        "val_input_ids, val_attention_masks = convert_to_tensor_batch(val_dataset)\n",
        "\n",
        "print(f\"Train input_ids shape: {train_input_ids.shape}\")\n",
        "print(f\"Train attention_masks shape: {train_attention_masks.shape}\")\n",
        "print(f\"Validation input_ids shape: {val_input_ids.shape}\")\n",
        "print(f\"Validation attention_masks shape: {val_attention_masks.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA5OTBR40EmU",
        "outputId": "30a5f826-3185-4f6c-c8fe-080de04d5439"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train input_ids shape: torch.Size([108887, 256])\n",
            "Train attention_masks shape: torch.Size([108887, 256])\n",
            "Validation input_ids shape: torch.Size([27222, 256])\n",
            "Validation attention_masks shape: torch.Size([27222, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_chunk(split):\n",
        "    dataset = train_input_ids if split == 'train' else val_input_ids\n",
        "\n",
        "    if len(dataset) < batch_size:\n",
        "        print(f\"Dataset too small for batch size ({batch_size}). Returning full dataset.\")\n",
        "        return dataset\n",
        "\n",
        "    start_pos = random.randint(0, len(dataset) - batch_size)\n",
        "\n",
        "    batch_samples = dataset[start_pos:start_pos + batch_size]\n",
        "\n",
        "    return batch_samples\n",
        "\n",
        "def get_batch(split):\n",
        "    data = get_random_chunk(split)\n",
        "\n",
        "    x = data[:, :-1]\n",
        "    y = data[:, 1:]\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = get_batch('train')\n",
        "x_val, y_val = get_batch('val')\n",
        "\n",
        "if x_train is not None and y_train is not None:\n",
        "    print(f\"Train batch x shape: {x_train.shape}\")\n",
        "    print(f\"Train batch y shape: {y_train.shape}\")\n",
        "if x_val is not None and y_val is not None:\n",
        "    print(f\"Validation batch x shape: {x_val.shape}\")\n",
        "    print(f\"Validation batch y shape: {y_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjvbEAOry2jy",
        "outputId": "5965e6b2-47b8-4902-863c-076204a60b21"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batch x shape: torch.Size([64, 255])\n",
            "Train batch y shape: torch.Size([64, 255])\n",
            "Validation batch x shape: torch.Size([64, 255])\n",
            "Validation batch y shape: torch.Size([64, 255])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "FQY0UH5W6wel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.sa(x)\n",
        "        x = self.ln1(x + y)\n",
        "        y = self.ffwd(x)\n",
        "        x = self.ln2(x + y)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def resize_token_embeddings(self, new_vocab_size):\n",
        "        old_embedding = self.token_embedding_table\n",
        "        self.token_embedding_table = nn.Embedding(new_vocab_size, old_embedding.embedding_dim)\n",
        "        self.token_embedding_table.weight.data[:old_embedding.num_embeddings] = old_embedding.weight.data\n",
        "\n",
        "        old_lm_head = self.lm_head\n",
        "        self.lm_head = nn.Linear(old_lm_head.in_features, new_vocab_size)\n",
        "        self.lm_head.weight.data[:old_lm_head.out_features] = old_lm_head.weight.data\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        B, T = index.shape\n",
        "\n",
        "        # print(f\"index shape: {index.shape}\")\n",
        "        # if targets is not None:\n",
        "        #    print(f\"targets shape: {targets.shape}\")\n",
        "\n",
        "        # print(f\"Max token ID: {index.max().item()}, Min token ID: {index.min().item()}\")\n",
        "        assert index.max().item() < vocab_size, f\"Token ID exceeds vocab_size ({vocab_size})\"\n",
        "        assert index.min().item() >= 0, \"Token ID is negative\"\n",
        "\n",
        "        assert T <= block_size, f\"Sequence length T ({T}) exceeds block_size ({block_size})\"\n",
        "\n",
        "        tok_emb = self.token_embedding_table(index)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(min(T, block_size), device=device))\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            index_cond = index[:, -block_size:]\n",
        "            logits, loss = self.forward(index_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            index_next = torch.multinomial(probs, num_samples=1)\n",
        "            index = torch.cat((index, index_next), dim=1)\n",
        "        return index\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "TbK68v_G60V-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "UC1jaqpW6_ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "WCoDgRHl7B3S"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'sep_token': '[SEP]'})\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "print(f\"Updated vocab size: {vocab_size}\")\n",
        "\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "\n",
        "model.resize_token_embeddings(vocab_size)\n",
        "\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKoxj5TgElPT",
        "outputId": "ff890141-9855-40a1-b511-15d36adfcb60"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated vocab size: 50259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"Final training loss: {loss.item()}\")\n",
        "\n",
        "torch.save(model.state_dict(), 'model-01.pt')\n",
        "print('Model saved as model-01.pt')\n"
      ],
      "metadata": {
        "id": "jf6NqEMl7EZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Usage\n"
      ],
      "metadata": {
        "id": "Fx9iwF3NPjHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel(vocab_size)\n",
        "model.load_state_dict(torch.load('model-01.pt'))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "prompt_question = \"How do I create a Python function to calculate factorial?\"\n",
        "\n",
        "inputs = tokenizer(prompt_question, return_tensors='pt', truncation=True, padding=True, max_length=block_size).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(inputs['input_ids'], max_new_tokens=200)\n",
        "\n",
        "generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated code:\", generated_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7FVd98vPmMy",
        "outputId": "fc5f277e-80f9-40cf-ee5b-888a58986327"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-525c772ff754>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model-01.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated code: How do I create a Python function to calculate factorial?  You can create a Python function named \"factorial\" to calculate factorial of a number. Here's an example:\n",
            "\n",
            "```python\n",
            "def factorial(n):\n",
            "    # Def loop through the string\n",
            "    print(n) # Loop until the given number is reached\n",
            "    # Calculate factorial of 1\n",
            "    for i in range(n+1, n+1):\n",
            "        for j in range(i+1, n+1):\n",
            "           print(i-1)\n",
            "    # Print the factorial of sys\n",
            "    return n * factorial(i+1)\n",
            "```\n",
            "\n",
            "In the provided code example, the function is `factorial` that takes an integer `n` as a parameter. It loops through the range from 2 to `n+1`. Inside the loop\n"
          ]
        }
      ]
    }
  ]
}